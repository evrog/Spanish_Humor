{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn, tensorflow.keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GRU, SimpleRNN, Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "word2vec = r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\SBW-vectors-300-min5.txt\"\n",
    "train=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\data\\haha_2019_train_preprocessed_lemmatized.csv\"\n",
    "test=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\data\\haha_2019_test_preprocessed_lemmatized.csv\"\n",
    "\n",
    "lda_train=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\lda+pos_neg\\train.csv\"\n",
    "lda_test=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\lda+pos_neg\\test.csv\"\n",
    "lda_ev=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\lda+pos_neg\\ev.csv\"\n",
    "\n",
    "emb_train=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\embeddings\\train.csv\"\n",
    "emb_test=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\embeddings\\test.csv\"\n",
    "emb_ev=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\embeddings\\ev.csv\"\n",
    "\n",
    "em_train=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\emotions\\train.csv\"\n",
    "em_test=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\emotions\\test.csv\"\n",
    "em_ev=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\emotions\\ev.csv\"\n",
    "\n",
    "other_train=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\other_features\\train.csv\"\n",
    "other_test=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\other_features\\test.csv\"\n",
    "other_ev=r\"C:\\Users\\Annie\\Documents\\Working\\Spanish jokes\\ALL_DATA\\other_features\\ev.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
    "    text = re.sub('[¡¿.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w)>=3]\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    text=' '.join(words)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(i) for i in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_train\n",
    "values= pd.read_csv(train, sep=',', header=None, encoding = 'utf-8-sig').values\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(values)\n",
    "#df=pd.DataFrame(values)\n",
    "\n",
    "m = len(values)\n",
    "\n",
    "train_length = int(0.9 * m)\n",
    "train_data, test_data = values[:train_length], values[train_length:]\n",
    "\n",
    "df=pd.DataFrame(train_data)\n",
    "\n",
    "texts_train=df[1].tolist()\n",
    "scores_train=df[9].tolist()\n",
    "categories_train_raw = [1 if str(s)!='nan' else 0 for s in scores_train]\n",
    "\n",
    "df=pd.DataFrame(test_data)\n",
    "\n",
    "texts_test=df[1].tolist()\n",
    "texts_test_original=df[1].tolist()\n",
    "scores_test=df[9].tolist()\n",
    "categories_test_raw = [1 if str(s)!='nan' else 0 for s in scores_test]\n",
    "\n",
    "\n",
    "df=pd.read_csv(test, sep=',', header=None, encoding = 'utf-8-sig')\n",
    "texts_ev=df[1].tolist()\n",
    "#texts_train= pd.read_csv(texts_ov, sep=';', header=None, encoding = 'utf-8-sig')[0].tolist()\n",
    "#categories_train_row= pd.read_csv(texts_ov, sep=';', header=None, encoding = 'utf-8-sig')[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda\n",
    "lda_scaler = MinMaxScaler()\n",
    "lda_values_train= pd.read_csv(lda_train, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "lda_values_train=lda_scaler.fit_transform(lda_values_train)\n",
    "lda_values_test= pd.read_csv(lda_test, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "lda_values_test=lda_scaler.transform(lda_values_test)\n",
    "lda_values_ev= pd.read_csv(lda_ev, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "lda_values_ev=lda_scaler.transform(lda_values_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings\n",
    "emb_scaler = MinMaxScaler()\n",
    "emb_values_train= pd.read_csv(emb_train, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "emb_values_train=emb_scaler.fit_transform(emb_values_train)\n",
    "emb_values_test= pd.read_csv(emb_test, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "emb_values_test=emb_scaler.transform(emb_values_test)\n",
    "emb_values_ev= pd.read_csv(emb_ev, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "emb_values_ev=emb_scaler.transform(emb_values_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotions\n",
    "em_scaler = MinMaxScaler()\n",
    "em_values_train= pd.read_csv(em_train, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "em_values_train=em_scaler.fit_transform(em_values_train)\n",
    "em_values_test= pd.read_csv(em_test, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "em_values_test=em_scaler.transform(em_values_test)\n",
    "em_values_ev= pd.read_csv(em_ev, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "em_values_ev=em_scaler.transform(em_values_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other\n",
    "other_scaler = MinMaxScaler()\n",
    "other_values_train= pd.read_csv(other_train, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "other_values_train=other_scaler.fit_transform(other_values_train)\n",
    "other_values_test= pd.read_csv(other_test, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "other_values_test=other_scaler.transform(other_values_test)\n",
    "other_values_ev= pd.read_csv(other_ev, sep=';', header=None, encoding = 'utf-8-sig').drop([0,1], axis=1).values\n",
    "other_values_ev=other_scaler.transform(other_values_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list=stopwords.words('spanish')\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "for i in range(len(texts_train)):\n",
    "    texts_train[i]=' '.join(clean_text(texts_train[i]))\n",
    "for i in range(len(texts_test)):\n",
    "    texts_test[i]=' '.join(clean_text(texts_test[i]))\n",
    "for i in range(len(texts_ev)):\n",
    "    texts_ev[i]=' '.join(clean_text(texts_ev[i]))\n",
    "    \n",
    "\n",
    "tfidf_train = vectorizer.fit_transform(texts_train).toarray()\n",
    "tfidf_test = vectorizer.transform(texts_test).toarray()\n",
    "tfidf_ev = vectorizer.transform(texts_ev).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=set()#set of all words\n",
    "for text in texts_train:\n",
    "    words_text=text.split();\n",
    "    words.update(words_text)\n",
    "for text in texts_test:\n",
    "    words_text=text.split();\n",
    "    words.update(words_text)\n",
    "for text in texts_ev:\n",
    "    words_text=text.split();\n",
    "    words.update(words_text)\n",
    "print(\"number of words: {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdict=dict()\n",
    "index=0\n",
    "\n",
    "with open(word2vec,'r',encoding = 'utf-8-sig')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word=str(f.readline()).replace('b','').replace('\\'','').replace('\\\\n','').lower().split()\n",
    "        w = stemmer.stem(word[0])\n",
    "        if w in words:\n",
    "            word.remove(word[0])\n",
    "            emb = [float(x) for x in word]\n",
    "            embdict[str(w)]=emb\n",
    "        index+=1\n",
    "        if index%100000==0:\n",
    "            print(\"iteration \"+str(index))\n",
    "\n",
    "print(\"size of dictionary: {0}\".format(len(embdict)))\n",
    "del(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(texts_train+texts_test+texts_ev)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embdict[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "del(embdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = tokenizer.texts_to_sequences(texts_train)\n",
    "texts_train = sequence.pad_sequences(texts_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_train.shape)\n",
    "texts_test = tokenizer.texts_to_sequences(texts_test)\n",
    "texts_test = sequence.pad_sequences(texts_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_test.shape)\n",
    "texts_ev = tokenizer.texts_to_sequences(texts_ev)\n",
    "texts_ev = sequence.pad_sequences(texts_ev, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', texts_ev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "categories_train = tf.keras.utils.to_categorical(categories_train_raw, num_classes)\n",
    "categories_test = tf.keras.utils.to_categorical(categories_test_raw, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cnn=Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "input_tfidf=Input(shape=(len(tfidf_train[0]),))\n",
    "input_em=Input(shape=(len(em_train[0]),))\n",
    "input_lda=Input(shape=(len(lda_train[0]),))\n",
    "input_other=Input(shape=(len(other_train[0]),))\n",
    "\n",
    "#cnn\n",
    "x = Embedding(29531, 300, input_length=250, trainable=True, name='300')(input_cnn)\n",
    "x=Conv1D(64,5,padding='same')(x)\n",
    "x=MaxPooling1D(pool_size = (20), strides=(10))(x)\n",
    "x=Conv1D(64,5,padding='same', name='64_1D')(x)\n",
    "x=MaxPooling1D(pool_size = (20), strides=(10), name='20')(x)\n",
    "x=Flatten(name='flatten')(x)\n",
    "x = Model(inputs=input_cnn, outputs=x)#64\n",
    "\n",
    "#tfidf 5000\n",
    "y = Dense(1024, activation='relu', name='1024')(input_tfidf)\n",
    "#y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation='relu')(y)\n",
    "#y = Dropout(0.1)(y)\n",
    "y = Dense(64, activation='relu', name='64')(y)\n",
    "#y = Dropout(0.1)(y)\n",
    "y = Model(inputs=input_tfidf, outputs=y)#64\n",
    "\n",
    "#emotions 63\n",
    "y2 = Dense(32, activation='relu', name='32')(input_em)\n",
    "#y2 = Dropout(0.1)(y2)\n",
    "y2 = Dense(16, activation='relu', name='16')(y2)\n",
    "#y2 = Dropout(0.1)(y2)\n",
    "y2 = Model(inputs=input_em, outputs=y2)#16\n",
    "\n",
    "#lda 12\n",
    "y3 = Dense(8, activation='relu')(input_lda)\n",
    "#y3 = Dropout(0.1)(y3)\n",
    "y3 = Dense(8, activation='relu')(y3)\n",
    "#y3 = Dropout(0.1)(y3)\n",
    "y3 = Model(inputs=input_lda, outputs=y3)#8\n",
    "\n",
    "combined=concatenate([x.output, y.output, y2.output, input_other], name='concat')#216\n",
    "z=Dense(128, activation='relu', name='128')(combined)\n",
    "#z = Dropout(0.1)(z)\n",
    "z=Dense(64, activation='relu')(z)\n",
    "z = Dropout(0.8)(z)\n",
    "z=Dense(2, activation='softmax', name='2')(z)\n",
    "\n",
    "model = tensorflow.keras.models.Model(inputs=[input_cnn, input_tfidf, input_em, input_other], outputs=z)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'],\n",
    "                  )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit([texts_train, np.array(tfidf_train), np.array(emb_values_train), np.array(em_values_train), np.array(lda_values_train), np.array(other_values_train)],\n",
    "          categories_train, epochs=1, \n",
    "          verbose=1, \n",
    "          validation_data=([texts_test, np.array(tfidf_test), np.array(emb_values_test), np.array(em_values_test), np.array(lda_values_test), np.array(other_values_test)], categories_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict([np.array(texts_test),np.array(values2), np.array(tfidf_test)]), axis=1)\n",
    "answer = np.argmax(categories_test, axis=1)\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict([np.array(texts_ev),np.array(values3),np.array(tfidf_ev)]), axis=1)\n",
    "print(predict)\n",
    "with open('prediction_cnn1.txt', 'w', encoding='utf-8') as file:\n",
    "    for p in predict:\n",
    "        print(str(p),file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
